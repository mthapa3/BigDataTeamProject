{"paragraphs":[{"text":"import scala.io.Source\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\nimport org.apache.spark.mllib.feature.{StandardScaler,Normalizer,ChiSqSelector}\nimport org.apache.spark.mllib.evaluation.{MulticlassMetrics, BinaryClassificationMetrics}\nimport org.apache.spark.sql.{Row, SQLContext}\nimport sqlContext.implicits._\nimport org.apache.spark.rdd.PairRDDFunctions\nimport org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\nimport org.apache.spark.mllib.linalg.SingularValueDecomposition\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.{SQLContext, DataFrame}\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.feature.PCA\nimport org.apache.spark.mllib.regression.LinearRegressionModel\nimport org.apache.spark.mllib.optimization.{L1Updater,SquaredL2Updater}\nimport org.apache.spark.mllib.regression.{LinearRegressionWithSGD,RidgeRegressionWithSGD,LassoWithSGD}\nimport org.apache.spark.mllib.evaluation.RegressionMetrics\nimport org.apache.commons.math3.distribution.ChiSquaredDistribution\nimport org.apache.commons.math3.distribution.MultivariateNormalDistribution\nimport org.apache.commons.math3.random.MersenneTwister\nimport org.apache.commons.math3.stat.correlation.Covariance\nimport scala.util\nimport org.apache.spark.mllib.feature.Normalizer","dateUpdated":"Apr 28, 2016 11:30:21 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461884860140_855697726","id":"20160428-230740_407630115","result":{"code":"SUCCESS","type":"TEXT","msg":"import scala.io.Source\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\nimport org.apache.spark.mllib.feature.{StandardScaler, Normalizer, ChiSqSelector}\nimport org.apache.spark.mllib.evaluation.{MulticlassMetrics, BinaryClassificationMetrics}\nimport org.apache.spark.sql.{Row, SQLContext}\nimport sqlContext.implicits._\nimport org.apache.spark.rdd.PairRDDFunctions\nimport org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\nimport org.apache.spark.mllib.linalg.SingularValueDecomposition\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.{SQLContext, DataFrame}\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.feature.PCA\nimport org.apache.spark.mllib.regression.LinearRegressionModel\nimport org.apache.spark.mllib.optimization.{L1Updater, SquaredL2Updater}\nimport org.apache.spark.mllib.regression.{LinearRegressionWithSGD, RidgeRegressionWithSGD, LassoWithSGD}\nimport org.apache.spark.mllib.evaluation.RegressionMetrics\nimport org.apache.commons.math3.distribution.ChiSquaredDistribution\nimport org.apache.commons.math3.distribution.MultivariateNormalDistribution\nimport org.apache.commons.math3.random.MersenneTwister\nimport org.apache.commons.math3.stat.correlation.Covariance\nimport scala.util\nimport org.apache.spark.mllib.feature.Normalizer\n"},"dateCreated":"Apr 28, 2016 11:07:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:50","dateFinished":"Apr 28, 2016 11:30:26 PM","dateStarted":"Apr 28, 2016 11:30:21 PM","focus":true},{"text":"val inputFileValue = z.input(\"File Name\")","dateUpdated":"Apr 28, 2016 11:30:35 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"File Name":"s3n://bigdate-neu/in/electronics.reviews.features.txt"},"forms":{"File Name":{"name":"File Name","displayName":"File Name","defaultValue":"","hidden":false}}},"jobName":"paragraph_1461884860142_856467223","id":"20160428-230740_388847350","result":{"code":"SUCCESS","type":"TEXT","msg":"inputFileValue: Object = s3n://bigdate-neu/in/electronics.reviews.features.txt\n"},"dateCreated":"Apr 28, 2016 11:07:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:51","dateFinished":"Apr 28, 2016 11:30:36 PM","dateStarted":"Apr 28, 2016 11:30:35 PM","focus":true},{"text":"val data = sc.textFile(inputFileValue.toString)","dateUpdated":"Apr 28, 2016 11:30:39 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461884860142_856467223","id":"20160428-230740_564673629","result":{"code":"SUCCESS","type":"TEXT","msg":"data: org.apache.spark.rdd.RDD[String] = s3n://bigdate-neu/in/electronics.reviews.features.txt MapPartitionsRDD[365] at textFile at <console>:135\n"},"dateCreated":"Apr 28, 2016 11:07:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:52","dateFinished":"Apr 28, 2016 11:30:39 PM","dateStarted":"Apr 28, 2016 11:30:39 PM","focus":true},{"text":"//data.collect()","dateUpdated":"Apr 28, 2016 11:07:40 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461884860142_856467223","id":"20160428-230740_739446510","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Apr 28, 2016 11:07:40 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:53"},{"text":"val parsedData = data.map { line =>\nval parts = line.split(\" \")\nLabeledPoint(parts(72).split(\"\"\"\\|\"\"\")(1).toDouble, Vectors.dense(parts.slice(3,71).map(x => x.split(\"\"\"\\|\"\"\")(1).toDouble).toArray))\n}\n","dateUpdated":"Apr 28, 2016 11:30:55 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461884860142_856467223","id":"20160428-230740_389628381","result":{"code":"SUCCESS","type":"TEXT","msg":"parsedData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[367] at map at <console>:137\n"},"dateCreated":"Apr 28, 2016 11:07:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:54","dateFinished":"Apr 28, 2016 11:30:55 PM","dateStarted":"Apr 28, 2016 11:30:55 PM","focus":true},{"text":"//parsedData.collect()\n//PCA\nval normalizer1 = new Normalizer()\nval normalizer2 = new Normalizer(p = Double.PositiveInfinity)\nval pca = new PCA(68).fit(parsedData.map(_.features))\nval projected = parsedData.map(p => p.copy(features = pca.transform(normalizer2.transform(p.features))))","dateUpdated":"Apr 28, 2016 11:30:57 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461884860142_856467223","id":"20160428-230740_806559764","result":{"code":"SUCCESS","type":"TEXT","msg":"normalizer1: org.apache.spark.mllib.feature.Normalizer = org.apache.spark.mllib.feature.Normalizer@435513e8\nnormalizer2: org.apache.spark.mllib.feature.Normalizer = org.apache.spark.mllib.feature.Normalizer@646b7bd8\npca: org.apache.spark.mllib.feature.PCAModel = org.apache.spark.mllib.feature.PCAModel@15d365b6\nprojected: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[377] at map at <console>:143\n"},"dateCreated":"Apr 28, 2016 11:07:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:55","dateFinished":"Apr 28, 2016 11:31:32 PM","dateStarted":"Apr 28, 2016 11:30:57 PM","focus":true},{"text":"val splits = projected.randomSplit(Array(0.8, 0.2), seed = 11L)\n\tval training = splits(0).cache()\n\tval test = splits(1).cache()","dateUpdated":"Apr 28, 2016 11:31:58 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461884860142_856467223","id":"20160428-230740_933173684","result":{"code":"SUCCESS","type":"TEXT","msg":"splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[378] at randomSplit at <console>:145, MapPartitionsRDD[379] at randomSplit at <console>:145)\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[378] at randomSplit at <console>:145\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[379] at randomSplit at <console>:145\n"},"dateCreated":"Apr 28, 2016 11:07:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56","dateFinished":"Apr 28, 2016 11:31:59 PM","dateStarted":"Apr 28, 2016 11:31:58 PM","focus":true},{"text":"%md Regression Models : Choose the algorithm","dateUpdated":"Apr 28, 2016 11:32:04 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461884860142_856467223","id":"20160428-230740_777032947","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Regression Models : Choose the algorithm</p>\n"},"dateCreated":"Apr 28, 2016 11:07:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:57","dateFinished":"Apr 28, 2016 11:32:04 PM","dateStarted":"Apr 28, 2016 11:32:04 PM","focus":true},{"text":"\nval algorithm = z.select(\"Algorithm\", Seq((\"LinearRegressionWithSGD_L0\",\"LinearRegressionWithSGD L0\"),\n                                    (\"LinearRegressionWithSGD_L1\",\"LinearRegressionWithSGD L1\"),\n                                    (\"LinearRegressionWithSGD_L2\",\"LinearRegressionWithSGD L2\"),\n                                    (\"Ridge_Regression\",\"Ridge Regression\"),\n                                    (\"Lasso_Regression\",\"Lasso Regression\")))\n","dateUpdated":"Apr 28, 2016 11:32:08 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"Algorithm":"Lasso_Regression"},"forms":{"Algorithm":{"name":"Algorithm","displayName":"Algorithm","defaultValue":"","options":[{"value":"LinearRegressionWithSGD_L0","displayName":"LinearRegressionWithSGD L0"},{"value":"LinearRegressionWithSGD_L1","displayName":"LinearRegressionWithSGD L1"},{"value":"LinearRegressionWithSGD_L2","displayName":"LinearRegressionWithSGD L2"},{"value":"Ridge_Regression","displayName":"Ridge Regression"},{"value":"Lasso_Regression","displayName":"Lasso Regression"}],"hidden":false}}},"jobName":"paragraph_1461884860143_856082475","id":"20160428-230740_744682822","result":{"code":"SUCCESS","type":"TEXT","msg":"algorithm: Object = Lasso_Regression\n"},"dateCreated":"Apr 28, 2016 11:07:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:58","dateFinished":"Apr 28, 2016 11:32:08 PM","dateStarted":"Apr 28, 2016 11:32:08 PM","focus":true},{"text":"// LinearRegressionWithSGD L0\nif(algorithm == \"LinearRegressionWithSGD_L0\"){\nval lreg = new LinearRegressionWithSGD()\nlreg.optimizer.setNumIterations(100).setStepSize(0.001)\nlreg.setIntercept(true)\nval model1 = lreg.run(training)\n\n// Training Data Prediction & Score\n val labelAndPreds = training.map { point =>\n    val prediction = model1.predict(point.features)\n    (point.label, prediction)\n    }\n    \nval trainingMetrics = new RegressionMetrics(labelAndPreds)\n// Get evaluation metrics.\nprintln(\"\\nTraining Data:Root Mean Squared Error using LinearRegressionWithSGD with L0 = \" + trainingMetrics.rootMeanSquaredError)    \n\n\n// Test Data Prediction & Score\n val labelAndPredsTest = test.map { point =>\n    val prediction = model1.predict(point.features)\n    (point.label, prediction)\n    }\n    \nval testMetrics = new RegressionMetrics(labelAndPredsTest)\n// Get evaluation metrics.\nprintln(\"\\nTest Data:Root Mean Squared Error using LinearRegressionWithSGD with L0 = \" + testMetrics.rootMeanSquaredError)    \n\n\n} \n\n// LinearRegressionWithSGD L1\nif(algorithm == \"LinearRegressionWithSGD_L1\"){\nval lreg = new LinearRegressionWithSGD()\nlreg.optimizer.setNumIterations(100).setUpdater(new L1Updater).setStepSize(0.001)\nlreg.setIntercept(true)\nval model1 = lreg.run(training)\n \n// Training Data Prediction & Score\n val  labelAndPreds = training.map { point =>\n    val prediction = model1.predict(point.features)\n    (point.label, prediction)\n    } \n    \nval trainingMetrics = new RegressionMetrics(labelAndPreds)\n// Get evaluation metrics.\nprintln(\"\\nTraining Data:Root Mean Squared Error using LinearRegressionWithSGD with L1 = \" + trainingMetrics.rootMeanSquaredError)\n\n\n// Test Data Prediction & Score\n val labelAndPredsTest = test.map { point =>\n    val prediction = model1.predict(point.features)\n    (point.label, prediction)\n    }\n    \nval testMetrics = new RegressionMetrics(labelAndPredsTest)\n// Get evaluation metrics.\nprintln(\"\\nTest Data:Root Mean Squared Error using LinearRegressionWithSGD with L1 = \" + testMetrics.rootMeanSquaredError)    \n\n\n}\n\n// LinearRegressionWithSGD L2\nif(algorithm == \"LinearRegressionWithSGD_L2\"){\nval lreg = new LinearRegressionWithSGD()\nlreg.optimizer.setNumIterations(100).setUpdater(new SquaredL2Updater).setStepSize(0.001)\nlreg.setIntercept(true)\nval model1 = lreg.run(training)\n\n// Training Data Prediction & Score\n val  labelAndPreds = test.map { point =>\n    val prediction = model1.predict(point.features)\n    (point.label, prediction)\n    }\n    \nval trainingMetrics = new RegressionMetrics(labelAndPreds)\n// Get evaluation metrics.\nprintln(\"\\nTraining Data:Root Mean Squared Error using LinearRegressionWithSGD with L2 = \" + trainingMetrics.rootMeanSquaredError)\n\n// Test Data Prediction & Score\n val labelAndPredsTest = test.map { point =>\n    val prediction = model1.predict(point.features)\n    (point.label, prediction)\n    }\n    \nval testMetrics = new RegressionMetrics(labelAndPredsTest)\n// Get evaluation metrics.\nprintln(\"\\nTest Data:Root Mean Squared Error using LinearRegressionWithSGD with L2 = \" + testMetrics.rootMeanSquaredError)    \n\n}\n\n//Ridge Regression\nif(algorithm == \"Ridge_Regression\"){\nval lreg = new RidgeRegressionWithSGD()\nlreg.optimizer.setNumIterations(100).setStepSize(0.001)\nlreg.setIntercept(true)\nval model2 = lreg.run(training)\n \n \n // Training Data Prediction & Score\n val  labelAndPreds = training.map { point =>\n    val prediction = model2.predict(point.features)\n    (point.label, prediction)\n    }\n\nval trainingMetrics = new RegressionMetrics(labelAndPreds)\n// Get evaluation metrics.\nprintln(\"\\nTraining Data:Root Mean Squared Error using Ridge Regression = \" + trainingMetrics.rootMeanSquaredError)\n\n// Test Data Prediction & Score\n val labelAndPredsTest = test.map { point =>\n    val prediction = model2.predict(point.features)\n    (point.label, prediction)\n    }\n    \nval testMetrics = new RegressionMetrics(labelAndPredsTest)\n// Get evaluation metrics.\nprintln(\"\\nTest Data:Root Mean Squared Error using Ridge Regression = \" + testMetrics.rootMeanSquaredError)    \n\n\n}\n\n//Lasso Regression\nif(algorithm == \"Lasso_Regression\"){\nprintln(\"Running Lasso Regression\")    \nval lreg = new LassoWithSGD()\nlreg.optimizer.setNumIterations(100).setStepSize(0.001)\nlreg.setIntercept(true)\nval model3 = lreg.run(training)\n  \n  // Training Data Prediction & Score\n val labelAndPreds = training.map { point =>\n    val prediction = model3.predict(point.features)\n    (point.label, prediction)\n    }\n    \nval trainingMetrics = new RegressionMetrics(labelAndPreds)\n// Get evaluation metrics.\nprintln(\"\\nTraining Data:Root Mean Squared Error using Lasso Regression = \" + trainingMetrics.rootMeanSquaredError)\n\n\n// Test Data Prediction & Score\n val labelAndPredsTest = test.map { point =>\n    val prediction = model3.predict(point.features)\n    (point.label, prediction)\n    }\n    \nval testMetrics = new RegressionMetrics(labelAndPredsTest)\n// Get evaluation metrics.\nprintln(\"\\nTest Data:Root Mean Squared Error using Ridge Regression = \" + testMetrics.rootMeanSquaredError)    \nmodel3.save(sc, \"s3n://bigdate-neu/in/lassoModel\")\n\n// model3.save(sc, \"/Users/Malika/Downloads/BIG_DATA_PROJECT_DATA/model/lassoModel\")\n}\n\n","dateUpdated":"Apr 28, 2016 11:34:21 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461884860143_856082475","id":"20160428-230740_349494638","result":{"code":"SUCCESS","type":"TEXT","msg":"Running Lasso Regression\n\nTraining Data:Root Mean Squared Error using Lasso Regression = 3.3124075533729758\n\nTest Data:Root Mean Squared Error using Ridge Regression = 3.3112259905610637\n"},"dateCreated":"Apr 28, 2016 11:07:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:59","dateFinished":"Apr 28, 2016 11:33:04 PM","dateStarted":"Apr 28, 2016 11:32:12 PM","focus":true},{"text":"/*\n1. Take a sample review\n2. Create a feature set based on LIWC\n3. load the LassoModel\n4. Input review featureset into the model and confirm prediction vs actual rating\n\n*/\n\n// EXAMPLE 1 : BAD REVIEW  Rating : 1.0 Prediction : 1.05\n// review text : They hurt your ears no mater what you do or try but they are nice and loud just not worth the money I would not bye them again\n\n\nval model = org.apache.spark.mllib.regression.LassoModel.load(sc,\"s3n://bigdate-neu/in/lassoModel\")\n\n// val model =  org.apache.spark.mllib.regression.LassoModel.load(sc,\"/Users/Malika/Downloads/BIG_DATA_PROJECT_DATA/model/lassoModel\")\n\n// val weights = model.weights(20)\n\nval featureData = \"AVW3EK4GUN6EF-B00004U8JS Dic|17 WC|22 achieve|2 adverb|0 affect|0 anger|0 anx|0 article|1 assent|0 auxverb|2 bio|1 body|0 cause|0 certain|0 cogmech|2 conj|1 death|0 discrep|0 excl|1 family|0 feel|1 filler|0 friend|0 funct|10 future|0 health|1 hear|0 home|0 humans|0 i|0 incl|1 ingest|0 inhib|0 insight|0 ipron|1 jeopardy|11 leisure|0 money|3 motion|0 negate|2 negative|1 negemo|0 nonfl|0 number|0 past|0 percept|1 posemo|0 positive|1 ppron|1 preps|4 present|2 pronoun|2 quant|0 relativ|0 relig|0 sad|0 satiq|0 see|0 sexual|0 shehe|0 social|1 space|0 swear|0 tentat|0 they|0 time|0 verb|2 we|0 wine|5 work|2 you|1 rating|1.0\" \n\n\nval test = { val parts = featureData.split(\" \")\nLabeledPoint(parts(72).split(\"\"\"\\|\"\"\")(1).toDouble, Vectors.dense(parts.slice(3,71).map(x => x.split(\"\"\"\\|\"\"\")(1).toDouble).toArray))\n}\n\nval (actualRating,prediction) =(test.label,model.predict(test.features))\n","dateUpdated":"Apr 28, 2016 11:33:22 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461884860143_856082475","id":"20160428-230740_588606321","result":{"code":"SUCCESS","type":"TEXT","msg":"model: org.apache.spark.mllib.regression.LassoModel = org.apache.spark.mllib.regression.LassoModel: intercept = 1.0005125062360185, numFeatures = 68\nfeatureData: String = AVW3EK4GUN6EF-B00004U8JS Dic|17 WC|22 achieve|2 adverb|0 affect|0 anger|0 anx|0 article|1 assent|0 auxverb|2 bio|1 body|0 cause|0 certain|0 cogmech|2 conj|1 death|0 discrep|0 excl|1 family|0 feel|1 filler|0 friend|0 funct|10 future|0 health|1 hear|0 home|0 humans|0 i|0 incl|1 ingest|0 inhib|0 insight|0 ipron|1 jeopardy|11 leisure|0 money|3 motion|0 negate|2 negative|1 negemo|0 nonfl|0 number|0 past|0 percept|1 posemo|0 positive|1 ppron|1 preps|4 present|2 pronoun|2 quant|0 relativ|0 relig|0 sad|0 satiq|0 see|0 sexual|0 shehe|0 social|1 space|0 swear|0 tentat|0 they|0 time|0 verb|2 we|0 wine|5 work|2 you|1 rating|1.0\ntest: org.apache.spark.mllib.regression.LabeledPoint = (1.0,[2.0,0.0,0.0,0.0,0.0,1.0,0.0,2.0,1.0,0.0,0.0,0.0,2.0,1.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,10.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,11.0,0.0,3.0,0.0,2.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,1.0,4.0,2.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,5.0,2.0])\nactualRating: Double = 1.0\nprediction: Double = 0.9988708138851908\n"},"dateCreated":"Apr 28, 2016 11:07:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:60","dateFinished":"Apr 28, 2016 11:33:26 PM","dateStarted":"Apr 28, 2016 11:33:22 PM","focus":true},{"text":"// Show weights of the model\nprintln(s\"Coefficients: ${model.weights} Intercept: ${model.intercept}\")","dateUpdated":"Apr 28, 2016 11:33:31 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461884860143_856082475","id":"20160428-230740_1396758965","result":{"code":"SUCCESS","type":"TEXT","msg":"Coefficients: [-7.971286455195175E-4,3.95798301532635E-6,3.883743512946383E-5,-3.7618679882525134E-5,1.3679604434590114E-5,-6.87233520961818E-6,-2.4412689605384826E-5,2.934640986236925E-6,-1.0758411576798614E-5,1.1218729589898636E-5,-1.88175681304894E-5,1.6707535669436428E-5,-8.059216945012231E-6,-2.4321595827704924E-6,-3.1540943020038352E-6,0.0,-4.99298250935981E-6,0.0,0.0,7.307849758855596E-7,-4.6120494386107174E-7,1.4768264176935595E-6,0.0,-3.1910335966298494E-6,-1.2993676593527065E-6,-1.0352277994194596E-6,-1.069218805820745E-6,3.5397959991573634E-6,1.3863246574637562E-6,1.5240711659480876E-7,2.4411195767594807E-6,0.0,0.0,-2.290113194566415E-6,-6.452551087859785E-7,-0.0,-4.265002155940473E-7,0.0,9.767090987021154E-8,-0.0,0.0,0.0,0.0,-0.0,-3.3490347917818384E-7,0.0,-0.0,0.0,-0.0,0.0,-0.0,0.0,0.0,-0.0,0.0,0.0,-0.0,-0.0,0.0,0.0,-0.0,-0.0,0.0,-0.0,-0.0,0.0,-0.0,0.0] Intercept: 1.0005125062360185\n"},"dateCreated":"Apr 28, 2016 11:07:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:61","dateFinished":"Apr 28, 2016 11:33:32 PM","dateStarted":"Apr 28, 2016 11:33:31 PM","focus":true},{"dateUpdated":"Apr 28, 2016 11:07:40 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1461884860143_856082475","id":"20160428-230740_1772185437","dateCreated":"Apr 28, 2016 11:07:40 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:63"}],"name":"Zeppelin-ML","id":"2BJDDREEQ","angularObjects":{"2B44YVSN1":[],"2AJXGMUUJ":[],"2AK8P7CPX":[],"2AM1YV5CU":[],"2AKK3QQXU":[],"2ANGGHHMQ":[]},"config":{"looknfeel":"default"},"info":{}}