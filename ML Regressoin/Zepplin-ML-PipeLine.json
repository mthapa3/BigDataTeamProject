{"paragraphs":[{"text":"import org.apache.spark.SparkContext\r\nimport org.apache.spark.SparkConf\r\nimport org.apache.spark.mllib.regression.LabeledPoint\r\nimport org.apache.spark.mllib.linalg.Vectors\r\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\r\nimport org.apache.spark.mllib.feature.{StandardScaler,Normalizer,ChiSqSelector}\r\nimport org.apache.spark.mllib.evaluation.{MulticlassMetrics, BinaryClassificationMetrics}\r\nimport org.apache.spark.sql.{Row, SQLContext}\r\nimport org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}\r\nimport sqlContext.implicits._\r\nimport org.apache.spark.rdd.PairRDDFunctions\r\nimport org.apache.spark.mllib.linalg.Matrix\r\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\r\nimport org.apache.spark.mllib.linalg.SingularValueDecomposition\r\nimport org.apache.spark.mllib.feature.PCA\r\nimport org.apache.spark.ml.{Pipeline, PipelineStage, Transformer}\r\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\r\nimport org.apache.spark.ml.classification.{DecisionTreeClassificationModel, DecisionTreeClassifier}\r\nimport org.apache.spark.ml.regression.LinearRegression \r\nimport org.apache.spark.ml.util.MetadataUtils\r\nimport org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, RegressionEvaluator}\r\nimport org.apache.spark.mllib.evaluation.{RegressionMetrics, MulticlassMetrics}\r\nimport org.apache.spark.rdd.RDD\r\nimport org.apache.spark.sql.types.StringType\r\nimport org.apache.spark.sql.{SQLContext, DataFrame}","dateUpdated":"Apr 17, 2016 8:00:28 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460937586538_485155020","id":"20160417-195946_1263788677","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\nimport org.apache.spark.mllib.feature.{StandardScaler, Normalizer, ChiSqSelector}\nimport org.apache.spark.mllib.evaluation.{MulticlassMetrics, BinaryClassificationMetrics}\nimport org.apache.spark.sql.{Row, SQLContext}\nimport org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}\nimport sqlContext.implicits._\nimport org.apache.spark.rdd.PairRDDFunctions\nimport org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\nimport org.apache.spark.mllib.linalg.SingularValueDecomposition\nimport org.apache.spark.mllib.feature.PCA\nimport org.apache.spark.ml.{Pipeline, PipelineStage, Transformer}\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\nimport org.apache.spark.ml.classification.{DecisionTreeClassificationModel, DecisionTreeClassifier}\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.util.MetadataUtils\nimport org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, RegressionEvaluator}\nimport org.apache.spark.mllib.evaluation.{RegressionMetrics, MulticlassMetrics}\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.{SQLContext, DataFrame}\n"},"dateCreated":"Apr 17, 2016 7:59:46 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:554","dateFinished":"Apr 17, 2016 8:00:36 PM","dateStarted":"Apr 17, 2016 8:00:28 PM","focus":true},{"text":"val inputFileValue = z.input(\"File Name\")","dateUpdated":"Apr 17, 2016 7:59:46 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"File Name":"/Users/rachanrhegde/BigData_FinalProject/S3/S3_data/out/part-00000"},"forms":{"File Name":{"name":"File Name","displayName":"File Name","defaultValue":"","hidden":false}}},"jobName":"paragraph_1460937586539_484770271","id":"20160417-195946_1673753703","result":{"code":"SUCCESS","type":"TEXT","msg":"inputFileValue: Object = /Users/rachanrhegde/BigData_FinalProject/S3/S3_data/out/part-00000\n"},"dateCreated":"Apr 17, 2016 7:59:46 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:555"},{"text":"val data = sc.textFile(inputFileValue.toString)","dateUpdated":"Apr 17, 2016 7:59:46 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460937586539_484770271","id":"20160417-195946_1903688261","result":{"code":"SUCCESS","type":"TEXT","msg":"data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4345] at textFile at <console>:221\n"},"dateCreated":"Apr 17, 2016 7:59:46 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:556"},{"text":"//data.collect()","dateUpdated":"Apr 17, 2016 7:59:46 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460937586540_482846526","id":"20160417-195946_1870207895","result":{"code":"SUCCESS","type":"TEXT","msg":"res66: Array[String] = Array(AKM1MP6P0OYPR-0132793040 Dic|29 WC|40 achieve|0 adverb|2 affect|2 anger|0 anx|0 article|2 assent|0 auxverb|2 bio|0 body|0 cause|0 certain|1 cogmech|7 conj|2 death|0 discrep|2 excl|4 family|0 feel|0 filler|0 friend|0 funct|18 future|0 health|0 hear|0 home|0 humans|1 i|0 incl|0 ingest|0 inhib|0 insight|1 ipron|1 jeopardy|17 leisure|2 money|3 motion|0 negate|0 negative|1 negemo|0 nonfl|0 number|0 past|0 percept|0 posemo|2 positive|2 ppron|2 preps|6 present|3 pronoun|3 quant|1 relativ|1 relig|0 sad|0 satiq|0 see|0 sexual|0 shehe|0 social|4 space|1 swear|0 tentat|1 they|0 time|0 verb|3 we|0 wine|11 work|1 you|2 rating|5.0, A2CX7LUOHB2NDG-0321732944 Dic|99 WC|126 achieve|5 adverb|7 affect|6 anger|2 anx|1 article|7 assent|0 auxverb|8 bio|0 body|0 cause|1 certain|4 ..."},"dateCreated":"Apr 17, 2016 7:59:46 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:557"},{"text":"val parsedData = data.map { line =>\nval parts = line.split(\" \")\nLabeledPoint(parts(72).split(\"\"\"\\|\"\"\")(1).toDouble, Vectors.dense(parts.slice(3,71).map(x => x.split(\"\"\"\\|\"\"\")(1).toDouble).toArray))\n}\n","dateUpdated":"Apr 17, 2016 7:59:46 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460937586540_482846526","id":"20160417-195946_996838199","result":{"code":"SUCCESS","type":"TEXT","msg":"parsedData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[2089] at map at <console>:219\n"},"dateCreated":"Apr 17, 2016 7:59:46 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:558"},{"text":"//parsedData.collect()\n//PCA\nval pca = new PCA(68).fit(parsedData.map(_.features))\nval projected = parsedData.map(p => p.copy(features = pca.transform(p.features)))","dateUpdated":"Apr 17, 2016 7:59:46 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460937586540_482846526","id":"20160417-195946_1139978353","result":{"code":"SUCCESS","type":"TEXT","msg":"pca: org.apache.spark.mllib.feature.PCAModel = org.apache.spark.mllib.feature.PCAModel@164185d2\nprojected: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[2093] at map at <console>:223\n"},"dateCreated":"Apr 17, 2016 7:59:46 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:559"},{"text":"val splits = projected.randomSplit(Array(0.8, 0.2), seed = 11L)\n\tval training = splits(0).cache()\n\tval test = splits(1).cache()","dateUpdated":"Apr 17, 2016 7:59:46 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460937586540_482846526","id":"20160417-195946_219795352","result":{"code":"SUCCESS","type":"TEXT","msg":"splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[3114] at randomSplit at <console>:225, MapPartitionsRDD[3115] at randomSplit at <console>:225)\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[3114] at randomSplit at <console>:225\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[3115] at randomSplit at <console>:225\n"},"dateCreated":"Apr 17, 2016 7:59:46 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:560"},{"text":"%md Regression Models : Choose the algorithm","dateUpdated":"Apr 17, 2016 7:59:46 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460937586540_482846526","id":"20160417-195946_714852367","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Regression Models : Choose the algorithm</p>\n"},"dateCreated":"Apr 17, 2016 7:59:46 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:561"},{"text":"\nval algorithm = z.select(\"Algorithm\", Seq((\"LinearRegressionWithSGD_L0\",\"LinearRegressionWithSGD L0\"),\n                                    (\"LinearRegressionWithSGD_L1\",\"LinearRegressionWithSGD L1\"),\n                                    (\"LinearRegressionWithSGD_L2\",\"LinearRegressionWithSGD L2\"),\n                                    (\"Ridge_Regression\",\"Ridge Regression\"),\n                                    (\"Lasso_Regression\",\"Lasso Regression\")))\n","dateUpdated":"Apr 17, 2016 7:59:46 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"Algorithm":"LinearRegressionWithSGD_L1"},"forms":{"Algorithm":{"name":"Algorithm","displayName":"Algorithm","defaultValue":"","options":[{"value":"LinearRegressionWithSGD_L0","displayName":"LinearRegressionWithSGD L0"},{"value":"LinearRegressionWithSGD_L1","displayName":"LinearRegressionWithSGD L1"},{"value":"LinearRegressionWithSGD_L2","displayName":"LinearRegressionWithSGD L2"},{"value":"Ridge_Regression","displayName":"Ridge Regression"},{"value":"Lasso_Regression","displayName":"Lasso Regression"}],"hidden":false}}},"jobName":"paragraph_1460937586540_482846526","id":"20160417-195946_1162333528","result":{"code":"SUCCESS","type":"TEXT","msg":"algorithm: Object = LinearRegressionWithSGD_L1\n"},"dateCreated":"Apr 17, 2016 7:59:46 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:562"},{"dateUpdated":"Apr 17, 2016 8:17:18 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460937586540_482846526","id":"20160417-195946_1346827346","result":{"code":"ERROR","type":"TEXT","msg":"lr: org.apache.spark.ml.regression.LinearRegression = linReg_d9ad805794b8\nres532: lr.type = linReg_d9ad805794b8\nres533: lr.type = linReg_d9ad805794b8\npipeline: org.apache.spark.ml.Pipeline = pipeline_6a1622b870bf\ncrossval: org.apache.spark.ml.tuning.CrossValidator = cv_22d99e270bf0\nres538: crossval.type = cv_22d99e270bf0\nres539: crossval.type = cv_22d99e270bf0\nparamGrid: org.apache.spark.ml.tuning.ParamGridBuilder = org.apache.spark.ml.tuning.ParamGridBuilder@270bd696\nres542: paramGrid.type = org.apache.spark.ml.tuning.ParamGridBuilder@270bd696\nres543: paramGrid.type = org.apache.spark.ml.tuning.ParamGridBuilder@270bd696\nres544: paramGrid.type = org.apache.spark.ml.tuning.ParamGridBuilder@270bd696\narrayMap: Array[org.apache.spark.ml.param.ParamMap] = \nArray({\n\tlinReg_d9ad805794b8-maxIter: 5,\n\tlinReg_d9ad805794b8-regParam: 0.1,\n\tlinReg_d9ad805794b8-tol: 1.0E-5\n}, {\n\tlinReg_d9ad805794b8-maxIter: 5,\n\tlinReg_d9ad805794b8-regParam: 0.1,\n\tlinReg_d9ad805794b8-tol: 0.001\n}, {\n\tlinReg_d9ad805794b8-maxIter: 5,\n\tlinReg_d9ad805794b8-regParam: 0.1,\n\tlinReg_d9ad805794b8-tol: 1.0E-4\n}, {\n\tlinReg_d9ad805794b8-maxIter: 10,\n\tlinReg_d9ad805794b8-regParam: 0.1,\n\tlinReg_d9ad805794b8-tol: 1.0E-5\n}, {\n\tlinReg_d9ad805794b8-maxIter: 10,\n\tlinReg_d9ad805794b8-regParam: 0.1,\n\tlinReg_d9ad805794b8-tol: 0.001\n}, {\n\tlinReg_d9ad805794b8-maxIter: 10,\n\tlinReg_d9ad805794b8-regParam: 0.1,\n\tlinReg_d9ad805794b8-tol: 1.0E-4\n}, {\n\tlinReg_d9ad805794b8-maxIter: 20,\n\tlinReg_d9ad805794b8-regParam: 0.1,\n\tlinReg_d9ad805794b8-...res545: crossval.type = cv_22d99e270bf0\nres546: crossval.type = cv_22d99e270bf0\norg.apache.spark.SparkException: Job 2306 cancelled part of cancelled job group zeppelin-20160417-195946_1346827346\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1136)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1113)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:75)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:180)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:67)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:90)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:71)\n\tat org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:144)\n\tat org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:140)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:42)\n\tat scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:43)\n\tat org.apache.spark.ml.Pipeline.fit(Pipeline.scala:140)\n\tat org.apache.spark.ml.Pipeline.fit(Pipeline.scala:91)\n\tat org.apache.spark.ml.Estimator.fit(Estimator.scala:59)\n\tat org.apache.spark.ml.Estimator$$anonfun$fit$1.apply(Estimator.scala:78)\n\tat org.apache.spark.ml.Estimator$$anonfun$fit$1.apply(Estimator.scala:78)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)\n\tat org.apache.spark.ml.Estimator.fit(Estimator.scala:78)\n\tat org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:104)\n\tat org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:99)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:99)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:260)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:265)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:267)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:269)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:271)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:273)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:275)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:277)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:279)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:281)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:283)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:285)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:287)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:289)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:291)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:293)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:295)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:297)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:299)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:301)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:303)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:305)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:307)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:309)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:311)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:313)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:315)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:317)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:319)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:321)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:323)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:325)\n\tat $iwC$$iwC$$iwC.<init>(<console>:327)\n\tat $iwC$$iwC.<init>(<console>:329)\n\tat $iwC.<init>(<console>:331)\n\tat <init>(<console>:333)\n\tat .<init>(<console>:337)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:674)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:667)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"},"dateCreated":"Apr 17, 2016 7:59:46 PM","status":"RUNNING","progressUpdateIntervalMs":500,"$$hashKey":"object:564","dateFinished":"Apr 17, 2016 8:17:17 PM","dateStarted":"Apr 17, 2016 8:17:18 PM","text":"//Linear regression\r\n\tval lr = new LinearRegression()\r\n\tlr.setMaxIter(10)\r\n\tlr.setRegParam(0.01)\r\n\r\n\t//Pipeline \r\n\tval pipeline = new Pipeline().setStages(Array(lr))\r\n\t\r\n\t//Cross validation\r\n\tval crossval = new CrossValidator()\r\n\tcrossval.setEstimator(pipeline)\r\n\tcrossval.setEvaluator(new RegressionEvaluator)\r\n\r\n\t//Parameter grid builder\r\n\tval paramGrid = new ParamGridBuilder()\r\n\tparamGrid.addGrid(lr.regParam, Array(0.1, 0.01))\r\n\tparamGrid.addGrid(lr.maxIter, Array(5, 10, 20))\r\n\tparamGrid.addGrid(lr.tol, Array(.00001,.001, .0001))\r\n\tval arrayMap = paramGrid.build()\r\n\tcrossval.setEstimatorParamMaps(arrayMap)\r\n\tcrossval.setNumFolds(4)\r\n\r\n\tval cvModel = crossval.fit(training.toDF)\r\n\t\r\n\t// Training Data Prediction & Score\r\n\tval trainPredictions = cvModel.transform(training.toDF()).select(\"prediction\", \"label\")\r\n\tval trainPredictionsAndLabels = trainPredictions.map {case Row(p: Double, l: Double) => (p, l)}\t\r\n\t\r\n\t// Get Training metrics\r\n\tval trainingMetrics = new RegressionMetrics(trainPredictionsAndLabels)\r\n\tprintln(\"\\nRoot Mean Squared Error = \" + trainingMetrics.rootMeanSquaredError)\r\n\r\n\t// Test Data Prediction & Score\r\n\tval testPredictions = cvModel.transform(test.toDF()).select(\"prediction\", \"label\")\r\n\tval testPredictionsAndLabels = testlabelAndPreds.map {case Row(p: Double, l: Double) => (p, l)}\r\n\r\n\t// Get Test Metrics\r\n\tval testMetrics = new RegressionMetrics(testPredictionsAndLabels)\r\n\tprintln(\"Root Mean Squared Error : \" + testMetrics.rootMeanSquaredError)\r\n\r\n","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460937683673_569753805","id":"20160417-200123_1793868276","dateCreated":"Apr 17, 2016 8:01:23 PM","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:768"}],"name":"Zepplin-ML-PipeLine","id":"2BHX9J5UK","angularObjects":{"2BG8BPDP9":[],"2BHNAV395":[],"2BGSFZYEE":[],"2BJX6426Q":[],"2BFSKPM5X":[],"2BJKHGQ3R":[],"2BHZQM1U7":[],"2BFX6W7GN":[],"2BJP5JC8X":[],"2BFZMKZSA":[],"2BH8UF92D":[],"2BG97XDQR":[],"2BHS1DUDZ":[],"2BJ1GKVSW":[]},"config":{"looknfeel":"default"},"info":{}}