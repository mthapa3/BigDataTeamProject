{"paragraphs":[{"text":"import scala.Predef._\nimport scala.io._\nimport scala.util.parsing.json\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.{SparkContext, SparkConf}\nimport org.apache.log4j.{Level, Logger}\nimport scala.util.Random\n\n","dateUpdated":"Apr 16, 2016 12:50:29 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460810948939_1011891794","id":"20160416-124908_68716809","result":{"code":"SUCCESS","type":"TEXT","msg":"import scala.Predef._\nimport scala.io._\nimport scala.util.parsing.json\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.{SparkContext, SparkConf}\nimport org.apache.log4j.{Level, Logger}\nimport scala.util.Random\n"},"dateCreated":"Apr 16, 2016 12:49:08 PM","dateStarted":"Apr 16, 2016 12:50:29 PM","dateFinished":"Apr 16, 2016 12:51:06 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:44"},{"text":"\n\n@serializable object ScalaJson {\n  def scalafy(entity: Any): Any = {\n       entity match {\n      case Some(x) => x\n      case None => \"?\"\n    }\n  }\n\n  def fromFile[A](path: String): A = {\n    println(path)\n    //val raw = scala.io.Source.fromFile(path).mkString\n    val raw = sc.textFile(path).reduce(_+_)\n    scalafy(json.JSON.parseFull(raw)).asInstanceOf[A]\n\n  }\n}","dateUpdated":"Apr 16, 2016 12:50:29 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460810948941_1009583301","id":"20160416-124908_1981540980","result":{"code":"SUCCESS","type":"TEXT","msg":"warning: there were 2 deprecation warning(s); re-run with -deprecation for details\ndefined module ScalaJson\n"},"dateCreated":"Apr 16, 2016 12:49:08 PM","dateStarted":"Apr 16, 2016 12:50:32 PM","dateFinished":"Apr 16, 2016 12:51:07 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:45"},{"text":"object Liwc {\n\n  val categories = List(\"funct\", \"pronoun\", \"ppron\", \"i\", \"we\", \"you\", \"shehe\", \"they\", \"ipron\", \"article\", \"verb\", \"auxverb\", \"past\", \"present\", \"future\", \"adverb\", \"preps\", \"conj\", \"negate\", \"quant\", \"number\", \"swear\", \"social\", \"family\", \"friend\", \"humans\", \"affect\", \"posemo\", \"negemo\", \"anx\", \"anger\", \"sad\", \"cogmech\", \"insight\", \"cause\", \"discrep\", \"tentat\", \"certain\", \"inhib\", \"incl\", \"excl\", \"percept\", \"see\", \"hear\", \"feel\", \"bio\", \"body\", \"health\", \"sexual\", \"ingest\", \"relativ\", \"motion\", \"space\", \"time\", \"work\", \"achieve\", \"leisure\", \"home\", \"money\", \"relig\", \"death\", \"assent\", \"nonfl\", \"filler\")\n\n  def _walk(token: String, index: Int, cursor: Map[String, Any]): List[String] = {\n    if (cursor.contains(\"*\")) {\n      // assert cursor(\"*\") = List[String]\n      return cursor(\"*\").asInstanceOf[List[String]]\n    }\n    else if (cursor.contains(\"$\") && index == token.size) {\n      return cursor(\"$\").asInstanceOf[List[String]]\n    }\n    else if (index < token.size) {\n      var letter = token(index).toString\n      if (cursor.contains(letter)) {\n        val nextCursor = cursor(letter).asInstanceOf[Map[String, Any]]\n        return _walk(token, index + 1, nextCursor)\n      }\n    }\n    return List()\n  }\n\n  // : Map[String, Int]\n  def apply(tokens: Seq[String], liwcWset:Map[String, Any]) = {\n    // returns a map from categories to counts\n    val categories = tokens.map(_walk(_, 0, liwcWset))\n    Map(\"Dic\" -> categories.count(_.size > 0), \"WC\" -> tokens.size) ++\n      categories.flatten.groupBy(identity).mapValues(_.size)\n  }\n\n}\n\n","dateUpdated":"Apr 16, 2016 12:50:29 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460810948941_1009583301","id":"20160416-124908_582853184","result":{"code":"SUCCESS","type":"TEXT","msg":"defined module Liwc\n"},"dateCreated":"Apr 16, 2016 12:49:08 PM","dateStarted":"Apr 16, 2016 12:51:07 PM","dateFinished":"Apr 16, 2016 12:51:10 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:46"},{"text":"\n    Logger.getRootLogger().setLevel(Level.ERROR)\n\n    val sqlContext = new SQLContext(sc)\n    val fulldfs:org.apache.spark.sql.DataFrame = sqlContext.read.json(\"s3n://rrh-bigdataproject/output.Electronics.strict.complete.json\")\n\n    val df = fulldfs.select(\"asin\",\"reviewerID\",\"reviewText\",\"overall\")\n","dateUpdated":"Apr 16, 2016 12:50:29 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460810948941_1009583301","id":"20160416-124908_1340549521","result":{"code":"SUCCESS","type":"TEXT","msg":"sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@6dee12d0\nfulldfs: org.apache.spark.sql.DataFrame = [asin: string, helpful: array<bigint>, overall: double, reviewText: string, reviewTime: string, reviewerID: string, reviewerName: string, summary: string, unixReviewTime: bigint]\ndf: org.apache.spark.sql.DataFrame = [asin: string, reviewerID: string, reviewText: string, overall: double]\n"},"dateCreated":"Apr 16, 2016 12:49:08 PM","dateStarted":"Apr 16, 2016 12:51:07 PM","dateFinished":"Apr 16, 2016 12:51:44 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:47"},{"text":"    \n   val positiveList:org.apache.spark.broadcast.Broadcast[Array[(String, Long)]]  = sc.broadcast(sc.textFile(\"s3n://rrh-bigdataproject/positive_words.txt\").zipWithIndex().collect())\n   val negativeList:org.apache.spark.broadcast.Broadcast[Array[(String, Long)]]  = sc.broadcast(sc.textFile(\"s3n://rrh-bigdataproject/negative_words.txt\").zipWithIndex().collect())\n   val satiqList:org.apache.spark.broadcast.Broadcast[Array[(String, Long)]] = sc.broadcast(sc.textFile(\"s3n://rrh-bigdataproject/sat_iq_words.txt\").zipWithIndex().collect())\n   val wineList:org.apache.spark.broadcast.Broadcast[Array[(String, Long)]] = sc.broadcast(sc.textFile(\"s3n://rrh-bigdataproject/wine_words.txt\").zipWithIndex().collect())\n   val jeoList:org.apache.spark.broadcast.Broadcast[Array[(String, Long)]] = sc.broadcast(sc.textFile(\"s3n://rrh-bigdataproject/jeo_words.txt\").zipWithIndex().collect())\n    val _trie : Map[String,Any] = {\n      ScalaJson.fromFile[Map[String, Any]](raw\"s3n://rrh-bigdataproject/liwc2007.trie\")\n    }\n    // println(\"LIWC-WordSet =>\"+_trie)\n    val liwcList:org.apache.spark.broadcast.Broadcast[Map[String, Any]] = sc.broadcast(_trie)\n\n    val categories = List(\"funct\", \"pronoun\", \"ppron\", \"i\", \"we\", \"you\", \"shehe\",\"they\", \"ipron\", \"article\", \"verb\", \"auxverb\", \"past\", \"present\", \"future\",\"adverb\", \"preps\", \"conj\", \"negate\", \"quant\", \"number\", \"swear\", \"social\",\"family\", \"friend\", \"humans\", \"affect\", \"posemo\", \"negemo\", \"anx\", \"anger\",\"sad\", \"cogmech\", \"insight\", \"cause\", \"discrep\", \"tentat\", \"certain\",\"inhib\", \"incl\", \"excl\", \"percept\", \"see\", \"hear\", \"feel\", \"bio\", \"body\",\"health\", \"sexual\", \"ingest\", \"relativ\", \"motion\", \"space\", \"time\", \"work\",\"achieve\", \"leisure\", \"home\", \"money\", \"relig\", \"death\", \"assent\", \"nonfl\",\"filler\")\n     val categoriesList:org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] = sc.broadcast(Set(categories: _*))","dateUpdated":"Apr 16, 2016 12:50:29 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460810948941_1009583301","id":"20160416-124908_1364858770","result":{"code":"SUCCESS","type":"TEXT","msg":"positiveList: org.apache.spark.broadcast.Broadcast[Array[(String, Long)]] = Broadcast(6)\nnegativeList: org.apache.spark.broadcast.Broadcast[Array[(String, Long)]] = Broadcast(10)\nsatiqList: org.apache.spark.broadcast.Broadcast[Array[(String, Long)]] = Broadcast(14)\nwineList: org.apache.spark.broadcast.Broadcast[Array[(String, Long)]] = Broadcast(18)\njeoList: org.apache.spark.broadcast.Broadcast[Array[(String, Long)]] = Broadcast(22)\ns3n://rrh-bigdataproject/liwc2007.trie\n_trie: Map[String,Any] = Map(e -> Map(s -> Map(l -> Map($ -> List(work)), p -> Map(e -> Map(c -> Map(i -> Map(a -> Map(l -> Map(l -> Map(y -> Map($ -> List(funct, adverb)))))))), r -> Map(e -> Map(s -> Map(s -> Map(o -> Map($ -> List(bio, ingest), * -> List(bio, ingest))))))), s -> Map(e -> Map(n -> Map(t -> Map(i -> Map(a -> Map(l -> Map($ -> List(cogmech, certain)))))))), t -> Map(r -> Map(o -> Map(g -> Map(e -> Map(n -> Map($ -> List(bio, health), * -> List(bio, health)))))))), x -> Map(e -> Map(c -> Map(u -> Map(t -> Map(i -> Map(o -> Map(n -> Map($ -> List(death), * -> List(death))), v -> Map(e -> Map($ -> List(work), * -> List(work))))))), r -> Map(c -> Map(i -> Map(s -> Map($ -> List(bio, health, leisure), * -> List(bio, health, leisure))))), s -> Map($ -> List(social, family))),...liwcList: org.apache.spark.broadcast.Broadcast[Map[String,Any]] = Broadcast(25)\ncategories: List[String] = List(funct, pronoun, ppron, i, we, you, shehe, they, ipron, article, verb, auxverb, past, present, future, adverb, preps, conj, negate, quant, number, swear, social, family, friend, humans, affect, posemo, negemo, anx, anger, sad, cogmech, insight, cause, discrep, tentat, certain, inhib, incl, excl, percept, see, hear, feel, bio, body, health, sexual, ingest, relativ, motion, space, time, work, achieve, leisure, home, money, relig, death, assent, nonfl, filler)\ncategoriesList: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] = Broadcast(26)\n"},"dateCreated":"Apr 16, 2016 12:49:08 PM","dateStarted":"Apr 16, 2016 12:51:10 PM","dateFinished":"Apr 16, 2016 12:51:57 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:48"},{"text":"def generateRecord(row:org.apache.spark.sql.Row): String ={\n    val posWset = positiveList.value.toMap\n    val negWset = negativeList.value.toMap\n    val satIQWset= satiqList.value.toMap\n    val wineWset= wineList.value.toMap\n    val jeoWset= jeoList.value.toMap\n    val liwcWset= liwcList.value\n    val categoriesSet= categoriesList.value\n\n    /*\n     * regular expression to remove special characters and split review text into words for scoring\n     */\n    val regex = \"[,.:;'\\\"\\\\?\\\\-!\\\\(\\\\)\\\\$]\".r\n    val temp = row(2).toString().replaceAll(\"\\\\$\",\"\")\n    val review = row(2).toString().split(\" \").flatMap(line => line.split(\"[\\\\s]\")).map(word => regex.replaceAllIn(word.trim.toLowerCase, \"\")).filter(word => !word.isEmpty)\n\n\n    val postiveCnt = review.map{word => posWset.get(word).size match { case 1 => 1 case 0 => 0}}.foldLeft(0)(_+_)\n    val negativeCnt = review.map{word => negWset.get(word).size match { case 1 => 1 case 0 => 0}}.foldLeft(0)(_+_)\n    val satiqCnt = review.map{word => satIQWset.get(word).size match { case 1 => 1 case 0 => 0}}.foldLeft(0)(_+_)\n    val wineCnt = review.map{word => wineWset.get(word).size match { case 1 => 1 case 0 => 0}}.foldLeft(0)(_+_)\n    val jeoCnt = review.map{word => jeoWset.get(word).size match { case 1 => 1 case 0 => 0}}.foldLeft(0)(_+_)\n    val liwcCnt = Liwc.apply(review,liwcWset)\n\n    val diffSet = categoriesSet.diff(liwcCnt.keySet)\n    val diffMap = diffSet.map(i => i -> 0).toMap\n    val interMap = Map(\"positive\" -> postiveCnt, \"negative\" -> negativeCnt, \"satiq\" -> satiqCnt, \"wine\" -> wineCnt, \"jeopardy\" -> jeoCnt)\n    val newMap = interMap ++ liwcCnt ++ diffMap\n    val finalMap = newMap.toSeq.sortBy((_._1))\n\n\n    val featureString = finalMap.toList.flatMap( a => List(a._1+\"|\"+a._2)).mkString(\" \")\n\n    (row(1)+\"-\"+row(0)+\" \"+featureString+\" \"+\"rating\"+\"|\"+row(3))\n\n  }","dateUpdated":"Apr 16, 2016 12:50:29 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460810948941_1009583301","id":"20160416-124908_1908513783","result":{"code":"SUCCESS","type":"TEXT","msg":"generateRecord: (row: org.apache.spark.sql.Row)String\n"},"dateCreated":"Apr 16, 2016 12:49:08 PM","dateStarted":"Apr 16, 2016 12:51:44 PM","dateFinished":"Apr 16, 2016 12:51:59 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:49"},{"text":"val records = df.map(generateRecord)","dateUpdated":"Apr 16, 2016 12:50:29 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460810948942_1010737548","id":"20160416-124908_1575399289","result":{"code":"SUCCESS","type":"TEXT","msg":"records: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[29] at map at <console>:71\n"},"dateCreated":"Apr 16, 2016 12:49:08 PM","dateStarted":"Apr 16, 2016 12:51:58 PM","dateFinished":"Apr 16, 2016 12:52:00 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:50"},{"text":" records.saveAsTextFile(\"s3n://rrh-bigdataproject/out/electronics/\")\n    println(\"Done\")","dateUpdated":"Apr 16, 2016 12:50:30 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460810948942_1010737548","id":"20160416-124908_1409966360","dateCreated":"Apr 16, 2016 12:49:08 PM","dateStarted":"Apr 16, 2016 12:52:00 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:51","dateFinished":"Apr 16, 2016 3:16:24 PM","result":{"code":"SUCCESS","type":"TEXT","msg":"Done\n"}},{"dateUpdated":"Apr 16, 2016 12:50:30 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460810948942_1010737548","id":"20160416-124908_1970051522","dateCreated":"Apr 16, 2016 12:49:08 PM","dateStarted":"Apr 16, 2016 12:52:00 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:52","dateFinished":"Apr 16, 2016 3:16:24 PM","result":{"code":"SUCCESS","type":"TEXT"}}],"name":"Zeppelin-FeatureGenerator","id":"2BK6K5YHA","angularObjects":{"2B44YVSN1":[],"2AJXGMUUJ":[],"2AK8P7CPX":[],"2AM1YV5CU":[],"2AKK3QQXU":[],"2ANGGHHMQ":[]},"config":{"looknfeel":"default"},"info":{}}